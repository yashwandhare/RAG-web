import asyncio
import aiohttp
import io
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urldefrag
from pypdf import PdfReader
from typing import List, Dict, Set, Optional
from app.core.config import settings
from app.core.logger import setup_logger

logger = setup_logger(__name__)

class WebCrawler:
    """Async web crawler with rate limiting, PDF support, and metadata extraction."""
    
    def __init__(self):
        self.visited: Set[str] = set()
        self.session: Optional[aiohttp.ClientSession] = None
        self.failed: Set[str] = set()
        
    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=settings.REQUEST_TIMEOUT)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={"User-Agent": settings.USER_AGENT}
        )
        return self
    
    async def __aexit__(self, *args):
        if self.session: await self.session.close()
    
    async def fetch_text(self, url: str) -> str:
        """Fetch content and extract text with metadata (Title)."""
        try:
            async with self.session.get(url) as response:
                response.raise_for_status()
                content_type = response.headers.get('Content-Type', '').lower()
                
                # Handle PDF
                if 'application/pdf' in content_type or url.endswith('.pdf'):
                    content = await response.read()
                    reader = PdfReader(io.BytesIO(content))
                    return "\n".join([p.extract_text() for p in reader.pages if p.extract_text()])
                
                # Handle HTML
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # CRITICAL: Extract Title for context
                title = soup.title.string.strip() if soup.title and soup.title.string else ""
                
                # Clean noise
                for tag in soup(["script", "style", "nav", "footer", "aside", "noscript"]): 
                    tag.decompose()
                
                body_text = " ".join(soup.get_text(" ", strip=True).split())
                
                # Return combined context
                if title:
                    return f"Page Title: {title}\n\n{body_text}"
                return body_text
                
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to fetch {url}: {e}")
            self.failed.add(url)
            return ""

    async def get_links(self, url: str, html: str, base_domain: str) -> List[str]:
        soup = BeautifulSoup(html, "html.parser")
        links = []
        for a in soup.find_all("a", href=True):
            abs_url = urljoin(url, a["href"]).split('#')[0]
            parsed = urlparse(abs_url)
            if parsed.netloc == base_domain and abs_url not in self.visited:
                links.append(abs_url)
        return links

    async def crawl(self, url: str, max_pages: int, depth: int = 1) -> List[Dict]:
        if depth > settings.MAX_CRAWL_DEPTH or len(self.visited) >= max_pages or url in self.visited:
            return []
        
        self.visited.add(url)
        logger.info(f"ğŸ•·ï¸ [{len(self.visited)}/{max_pages}] Crawling: {url}")
        
        text = await self.fetch_text(url)
        if not text or len(text) < 100: return []
        
        pages = [{"url": url, "text": text, "depth": depth}]
        
        if depth < settings.MAX_CRAWL_DEPTH:
            try:
                # Re-fetch HTML for link extraction (cached in memory usually, or just request again)
                # Ideally pass HTML from fetch_text, but for simplicity we re-parse or extract in one go.
                # Here we used fetch_text which returns string. 
                # Optimization: We assume the crawler is fast enough. 
                # For a strictly correct implementation without re-fetching, 
                # we'd need to refactor fetch_text to return (text, html).
                # To keep it simple and working: we limit link extraction for now or assume simple crawl.
                # NOTE: For hackathon speed, we'll just crawl top links blindly if we can't parse links from text.
                # Actually, let's just create a session.get here for links to ensure robust crawling.
                async with self.session.get(url) as resp:
                     html_content = await resp.text()
                     links = await self.get_links(url, html_content, urlparse(url).netloc)
                     
                tasks = [self.crawl(link, max_pages, depth + 1) for link in links[:5]]
                results = await asyncio.gather(*tasks)
                for res in results: pages.extend(res)
            except:
                pass # Ignore link extraction errors
            
        return pages[:max_pages]

async def crawl_site_async(url: str, max_pages: int = 10, max_depth: int = 3):
    async with WebCrawler() as crawler:
        return await crawler.crawl(url, max_pages, depth=1)